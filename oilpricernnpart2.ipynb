{"nbformat_minor": 1, "cells": [{"source": "# From Keras RNN to scoring with the Watson Machine learning Python client\n\nThis notebook continues working with the RNN developed  in [Predicting Oil Prices Using an RNN in Watson Studio](https://github.com/djccarew/timeseries-rnn-lab-part1). It  contains  the steps and code to demonstrate support of deep learning experiments in Watson Machine Learning Service based on the  RNN developed previously.  It  introduces commands for getting data, training_definition persistence, hyper parameter optimization, model persistence, model deployment and scoring.\n\nThis notebook is based on the example notebook\n[From keras experiment to scoring with watson-machine-learning-client](https://dataplatform.ibm.com/analytics/notebooks/v2/1c9801fc-5063-4564-a756-75e99be47cd0/view?access_token=d38aa735e323be34260be5fcf65813cea1f5f8a17a256e1d2f23796fdcd11a7d) which follows more or less the same steps with a model based on the MNIST handwriting digits dataset.\n\n## 1. Setup the environment\n\nBefore starting to run the code in this notebook, you must perform the following setup tasks:\n\n\ni. Create a Watson Machine Learning Service instance and associate it with the Watson Studio project that contains this notebook. Information on how to do this is [here](https://github.com/djccarew/timeseries-rnn-lab-part2)\n\nii. Add specific credentials to the Cloud Object Storage instance associated with the Watson Studio project that contains this notebook. Information on how to do this is [here](https://github.com/djccarew/timeseries-rnn-lab-part2)\n\niii. Copy the credentials to a text file so that they can be easily copied to this notebook. Information on how to do this is  [here](https://github.com/djccarew/timeseries-rnn-lab-part2)\n\n\n### 1.1 Work with Cloud Object Storage(COS)\n\nImport the boto library, which allows Python developers to manage Cloud Object Storage.", "cell_type": "markdown", "metadata": {}}, {"source": " # Some required imports\nimport ibm_boto3\nfrom ibm_botocore.client import Config\nimport os\nimport json\nimport warnings\nimport time", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Add your COS credentials.\n\nCopy the credentials  you saved to a text file during setup into the cell below. Note that the variable ```cos_credentials``` is a Python dictionary and should be defined with your credentials as follows:\n\n```\ncos_credentials = {\n  \"apikey\": \"___\",\n  \"cos_hmac_keys\": {\n    \"access_key_id\": \"___\",\n    \"secret_access_key\": \"___\"\n  },\n  \"endpoints\": \"https://cos-service.bluemix.net/endpoints\",\n  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:cloud-object-storage:global:a/d86af7367f70fba4f306d3c19c469d89:6244216d-4578-4ac4-a6d8-baca423111f9::\",\n  \"iam_apikey_name\": \"auto-generated-apikey-5ed63735-bc55-4c4d-8cc2-8ac6b38f554d\",\n  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/d86af7367f70fba4f306d3c19c469d89::serviceid:ServiceId-2c690700-a604-4ef3-b11b-34966debc9b2\",\n  \"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/d86af7367f70fba4f306d3c19c469d89:6244216d-4578-4ac4-a6d8-baca423111f9::\"\n}\n\n```", "cell_type": "markdown", "metadata": {}}, {"source": "# Copy and paste your Cloud Object Storage credentials here\n## Start COS credentials\ncos_credentials = {\n\n}\n## End COS credentials\n\napi_key = cos_credentials['apikey']\nservice_instance_id = cos_credentials['resource_instance_id']\nauth_endpoint = 'https://iam.bluemix.net/oidc/token'\nservice_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Initialize the Cloud Object Storage (COS) client", "cell_type": "markdown", "metadata": {}}, {"source": "cos = ibm_boto3.resource('s3',\n                         ibm_api_key_id=api_key,\n                         ibm_service_instance_id=service_instance_id,\n                         ibm_auth_endpoint=auth_endpoint,\n                         config=Config(signature_version='oauth'),\n                         endpoint_url=service_endpoint)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Create the buckets needed to store training data and training results. \n\n**Note:** Bucket names have to be unique - please rename the buckets you create if needed to ensure uniqueness.", "cell_type": "markdown", "metadata": {}}, {"source": "buckets = ['oilprice-rnn-data', 'oilprice-rnn-results']\nfor bucket in buckets:\n    if not cos.Bucket(bucket) in cos.buckets.all():\n        print('Creating bucket \"{}\"...'.format(bucket))\n        try:\n            cos.create_bucket(Bucket=bucket)\n        except ibm_boto3.exceptions.ibm_botocore.client.ClientError as e:\n            print('Error: {}.'.format(e.response['Error']['Message']))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Now we should have our buckets created.", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "print(list(cos.buckets.all()))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### 1.2 Downloading oil price  data and uploading  it to COS buckets\u00b6\nWe will work with the weekly  oil prices for West Texas crude. Let's download the training data and upload  to the ``` oilprice-rnn-data``` bucket.\n\nRun the code in the cell below to create the ```OILPRICE_RNN_DATA``` folder and download the data  file from the github repository.\n", "cell_type": "markdown", "metadata": {}}, {"source": "!pip install wget\nimport wget, os\n\nlink = 'https://raw.githubusercontent.com/djccarew/timeseries-rnn-lab-part1/master/data/WCOILWTICO.csv'\n\ndata_dir = 'OILPRICE_RNN_DATA'\nif not os.path.isdir(data_dir):\n    os.mkdir(data_dir)\n\nif not os.path.isfile(os.path.join(data_dir, os.path.join(link.split('/')[-1]))):\n    wget.download(link, out=data_dir)  \n    \ndata_file_path = os.path.join(data_dir, os.path.join(link.split('/')[-1]))\n        \n!ls OILPRICE_RNN_DATA\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Upload the data file to the  Cloud Object Storage bucket you just created", "cell_type": "markdown", "metadata": {}}, {"source": "bucket_name = buckets[0]\nbucket_obj = cos.Bucket(bucket_name)\n\nfor filename in os.listdir(data_dir):\n    with open(os.path.join(data_dir, filename), 'rb') as data: \n        bucket_obj.upload_file(os.path.join(data_dir, filename), filename)\n        print('{} is uploaded.'.format(filename))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Verify that the data file was uploaded to Cloud Object Storage", "cell_type": "markdown", "metadata": {}}, {"source": "for obj in bucket_obj.objects.all():\n    print('Object key: {}'.format(obj.key))\n    print('Object size (kb): {}'.format(obj.size/1024))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "We are done with Cloud Object Storage, we are ready to train our model.\n\n### 1.3 Work with the Watson Machine Learning instance", "cell_type": "markdown", "metadata": {}}, {"source": "# Required imports\nimport urllib3, requests, json, base64, time, os\nwarnings.filterwarnings('ignore')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Authenticate to the Watson Machine Learning service on IBM Cloud.\n\n**Note:** Copy the Watson Machine Learning service  credentials  you saved to a text file during setup into the cell below. Note that the variable ```wml_credentials``` is a Python dictionary and should be defined with your credentials as follows:\n\n```\nwml_credentials = {\n  \"url\": \"https://ibm-watson-ml.mybluemix.net\",\n  \"username\": \"___\",\n  \"password\": \"___\",\n  \"instance_id\": \"___\"\n}\n```", "cell_type": "markdown", "metadata": {}}, {"source": "# Copy and paste your Cloud Object Storage credentials here\n## Start WML service credentials\nwml_credentials = {\n\n}\n## End WML service credentials\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "**Install watson-machine-learning-client from pypi**", "cell_type": "markdown", "metadata": {}}, {"source": "!pip install --upgrade watson-machine-learning-client", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "**Import watson-machine-learning-client and authenticate to service instance**", "cell_type": "markdown", "metadata": {}}, {"source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient\nclient = WatsonMachineLearningAPIClient(wml_credentials)\nprint(client.version)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## 2. Training definitions\n\nFor purpose of this example one Keras model definition of an RNN has been prepared.\n\n### 2.1 Save training definition\nPrepare training definition metadata", "cell_type": "markdown", "metadata": {}}, {"source": "model_definition_metadata = {\n            client.repository.DefinitionMetaNames.NAME: \"OILPRICE-RNN\",\n           client.repository.DefinitionMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n            client.repository.DefinitionMetaNames.FRAMEWORK_VERSION: \"1.5\",\n            client.repository.DefinitionMetaNames.RUNTIME_NAME: \"python\",\n            client.repository.DefinitionMetaNames.RUNTIME_VERSION: \"3.5\",\n            client.repository.DefinitionMetaNames.EXECUTION_COMMAND: \"python3 oilprice_rnn.py\"\n            }", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "**Get sample model definition content files from git (python keras script with RNN)**", "cell_type": "markdown", "metadata": {}}, {"source": "model_filename = 'oilprice_rnnV2.zip'\n\nif os.path.isfile(model_filename):\n    !ls 'oilprice_rnnV2.zip'\nelse:\n    !wget https://github.com/djccarew/timeseries-rnn-lab-part2/raw/master/model-source/oilprice_rnnV2.zip\n    !ls 'oilprice_rnnV2.zip'", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "**Publish training definition in Watson Machine Learning repository**", "cell_type": "markdown", "metadata": {}}, {"source": "definition_details = client.repository.store_definition(model_filename, model_definition_metadata)\n\ndefinition_url = client.repository.get_definition_url(definition_details)\ndefinition_uid = client.repository.get_definition_uid(definition_details)\nprint(definition_url)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## 3. Experiment definition\n\n### 3.1 Save experiment\n\n**Experiment configuration dictionary**\n\nCreate experiment that will train models based on previously stored definitions.\n\n\nTRAINING_DATA_REFERENCE - location of traininng data", "cell_type": "markdown", "metadata": {}}, {"source": "TRAINING_DATA_REFERENCE = {\n                            \"connection\": {\n                                \"endpoint_url\": service_endpoint,\n                                \"aws_access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n                                \"aws_secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n                            },\n                            \"source\": {\n                                \"bucket\": buckets[0],\n                            },\n                            \"type\": \"s3\"\n}\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "\nTRAINING_RESULTS_REFERENCE - location of training results\n", "cell_type": "markdown", "metadata": {}}, {"source": "TRAINING_RESULTS_REFERENCE = {\n                                \"connection\": {\n                                    \"endpoint_url\": service_endpoint,\n                                    \"aws_access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n                                    \"aws_secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n                                },\n                                \"target\": {\n                                    \"bucket\": buckets[1],\n                                },\n                                \"type\": \"s3\"\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Configure the hyper parameters optimizer for your experiment. The  objective is to find the combination of hyper parameters that minimizes the *val_loss* metric (i.e. mean squared error of the test data set)  so it is indicated  as the optimizer objective. \n\nThe two hyper parameters that are to be optimized are:\n\ni. **dropout_rate** - the dropout rate for the  Dropout layer in the model \n\nii. **prev_periods** - the number of weeks of data to use to predict the next week's price . If set to 1, the input for the prediction  for *week n* will  be the price for *week n-1*. If set to 2, the input for the prediction  for *week n* will  be the prices for *week n-2* and *week n-1* and so on.\n\n**num_optimizer_steps** tells the optimizer how many models we want to train based on hyper parameter value combinations. Here 6 are used since we have 6 possible  combinations of hyper parameter values", "cell_type": "markdown", "metadata": {}}, {"source": "HPO = {\n        \"method\": {\n            \"name\": \"random\",\n            \"parameters\": [\n                client.experiments.HPOMethodParam(\"objective\", \"val_loss\"),\n                client.experiments.HPOMethodParam(\"maximize_or_minimize\", \"minimize\"),\n                client.experiments.HPOMethodParam(\"num_optimizer_steps\", 6)\n            ]\n        },\n        \"hyper_parameters\": [\n            client.experiments.HPOParameter('dropout_rate', min=0.1, max=0.5, step=0.2),\n            client.experiments.HPOParameter('prev_periods', min=1, max=2, step=1)\n        ]\n     }       ", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Configure your experiment. The experiment metadata links previously stored training definitions and provides information about compute_configuration that will be used to run the training.", "cell_type": "markdown", "metadata": {}}, {"source": "experiment_metadata = {\n            client.repository.ExperimentMetaNames.NAME: \"Oil Price RNN Experiment\",\n            client.repository.ExperimentMetaNames.DESCRIPTION: \"Best model for RNN oil price forecaster.\",\n            client.repository.ExperimentMetaNames.AUTHOR_EMAIL: \"yourname@youremail.com\",\n            client.repository.ExperimentMetaNames.EVALUATION_METRICS: [\"mae\"],\n            client.repository.ExperimentMetaNames.TRAINING_DATA_REFERENCE: TRAINING_DATA_REFERENCE,\n            client.repository.ExperimentMetaNames.TRAINING_RESULTS_REFERENCE: TRAINING_RESULTS_REFERENCE,\n            client.repository.ExperimentMetaNames.TRAINING_REFERENCES: [\n                        {\n                            \"name\": \"OILPRICE_RNN\",\n                            \"training_definition_url\": definition_url,\n                            \"compute_configuration\": {\"name\": \"k80x2\"},\n                            \"hyper_parameters_optimization\": HPO\n                            \n                        }],\n            }", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "**Store experiment in Watson Machine Learning repository**", "cell_type": "markdown", "metadata": {}}, {"source": "experiment_details = client.repository.store_experiment(meta_props=experiment_metadata)\n\nexperiment_uid = client.repository.get_experiment_uid(experiment_details)\nprint(experiment_uid)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## 4. Run experiment\n\n### 4.1 Running experiments\n\nThis kicks off the  experiment asynchronously. You'll have to monitor its progress below to know when it has completed", "cell_type": "markdown", "metadata": {}}, {"source": "experiment_run_details = client.experiments.run(experiment_uid, asynchronous=True)\nexperiment_run_uid = client.experiments.get_run_uid(experiment_run_details)\nprint(experiment_run_uid)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Keep running the cell below periodically  until all the training runs are in the COMPLETED state as shown below:\n```\n--------------------  ------------  ---------  --------------------  --------------------  ...\nGUID (training)       NAME          STATE      SUBMITTED             FINISHED              ...\ntraining-vw7UqMZiR    OILPRICE_RNN  completed  2018-04-14T13:46:10Z  2018-04-14T13:53:47Z  ...\ntraining-vw7UqMZiR_0  OILPRICE_RNN  completed  2018-04-14T13:47:22Z  -                     ...\n                                                                                           ...\ntraining-vw7UqMZiR_1  OILPRICE_RNN  completed  2018-04-14T13:47:22Z  -                     ...\n                                                                                           ...\ntraining-vw7UqMZiR_2  OILPRICE_RNN  completed  2018-04-14T13:47:22Z  -                     ...\n                                                                                           ...\ntraining-vw7UqMZiR_3  OILPRICE_RNN  completed  2018-04-14T13:47:22Z  -                     ...\n                                                                                           ...\ntraining-vw7UqMZiR_4  OILPRICE_RNN  completed  2018-04-14T13:47:22Z  -                     ...\n                                                                                           ...\ntraining-vw7UqMZiR_5  OILPRICE_RNN  completed  2018-04-14T13:47:22Z  -                     ...\n                                                                                           ...\n--------------------  ------------  ---------  --------------------  --------------------  ...\n```", "cell_type": "markdown", "metadata": {}}, {"source": "# Keep running this cell periodically  until all the training runs are in the COMPLETED state as illustrated above:\nclient.experiments.list_training_runs(experiment_run_uid)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Once the experiment is completed, the next order of business is to find out which training run performed the best and what are the corresponding hyper parameters for that run.\n\nAll that infomation is available via the ```client.experiments.get_run_details(...)``` call", "cell_type": "markdown", "metadata": {}}, {"source": "experiment_run_details = client.experiments.get_run_details(experiment_run_uid)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### 4.2 Assessing the results\n\nRather than navigate through the reams of information about the experiment, lets put the  stuff we're interested  in into a Data Frame so it's easier to work with. We'll get the results of each training run and the hyper parameters values used.\n\n**Note:** In practice you could export this to a CSV file or stick it in a database so you can peruse it later at your leisure.", "cell_type": "markdown", "metadata": {}}, {"source": "import pandas as pd\nrows_list = []\nfor m in experiment_run_details['entity']['training_statuses']:\n    if len (m['metrics']) > 0:\n        for l in m['metrics'][-2:]:\n           if l['phase'] == 'test':\n              last_metric = l\n              break\n        for h in m['hyper_parameters']:\n            if h['name'] == 'dropout_rate':\n               dropout_rate = h['double_value']\n            else:\n               prev_periods = h['int_value']\n        for v in last_metric['values']:\n            if v['name'] == 'val_loss':\n               val_loss = v['value']\n            else:\n               val_mae = v['value']\n        one_row = [m['training_guid'],  last_metric['phase'], val_mae, val_loss, dropout_rate,  prev_periods]\n        rows_list.append(one_row)\n            \nmetrics_df = pd.DataFrame(rows_list,columns=['GUID', 'PHASE', 'MAE', 'VAL LOSS', 'DROPOUT', 'PREV PERIODS'])\nmetrics_df", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "And the winner is ?????\n\nLook for the run that had the lowest validation loss (ie mean squared error) on the test data", "cell_type": "markdown", "metadata": {}}, {"source": "best_run_df = metrics_df.nsmallest(1, 'VAL LOSS')\nbest_run_df", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Save the pertinent details of the best run so we can deploy it in the next step", "cell_type": "markdown", "metadata": {}}, {"source": "best_model_guid = best_run_df['GUID'].iloc[0]\nbest_prev_periods = best_run_df['PREV PERIODS'].iloc[0]\nprint('Best run GUID ' + str(best_model_guid ))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## 5. Create online deployment\nYou can deploy the  stored model as a webservice (online) using the Watson Machine Learning service API\n\n### 5.1 Store trained model\nSave the model in the Watson Machine Learning repository", "cell_type": "markdown", "metadata": {}}, {"source": "saved_model_details = client.repository.store_model(best_model_guid, {'name': 'Oil price RNN best model'})\nmodel_guid = client.repository.get_model_uid(saved_model_details)\nprint(\"Saved model guid: \" + model_guid)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### 5.2 Create online deployment from stored model", "cell_type": "markdown", "metadata": {}}, {"source": "deployment_details = client.deployments.create(name=\"Oil price RNN deployment\", model_uid=model_guid)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Extract scoring endpoint from deployment details", "cell_type": "markdown", "metadata": {}}, {"source": "scoring_url = client.deployments.get_scoring_url(deployment_details)\nprint(scoring_url)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## 6. Scoring\n\nPrepare sample scoring data to score the  deployed model\n\nWe'll use the last week(s) of data in the dataset as input to predict the price of the week of 4/6/2018 (the last week covered by the data set is 3/30/2018). \n\nThe input node of our model expects data with shape *(1, n)* where *n* is the value of the hyper parameter ```prev_periods``` so we need to make sure our data to be scored is shaped accordingly", "cell_type": "markdown", "metadata": {}}, {"source": "import numpy as np\nprices_df = pd.read_csv(data_file_path, index_col='DATE')\nlast_prices = prices_df.tail(best_prev_periods)['WCOILWTICO'].values.reshape(-1,1).astype('float32')\nlast_prices = np.reshape(last_prices, (1, best_prev_periods))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Prepare scoring payload and score.", "cell_type": "markdown", "metadata": {}}, {"source": "scoring_data = {'values': [last_prices.tolist()]}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "predictions = client.deployments.score(scoring_url, scoring_data)\nprint(\"Scoring result: \" + str(predictions))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## 7. Summary and next steps\n\nYou successfully completed this notebook! You learned how to use the watson-machine-learning-client to run experiments. Check out the [Online Documentation](https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html) for more samples, tutorials, documentation, how-tos, and blog posts.\n\nCopyright \u00a9 2017, 2018 IBM. This notebook and its source code are released under the terms of the MIT License", "cell_type": "markdown", "metadata": {}}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.4", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}